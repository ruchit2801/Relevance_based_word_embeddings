{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relevance Based Word Embeddings \n",
    "\n",
    "This notebook implements the model for training relevance based word embeddings. The model is maximum likelihood model which is based on bag-of-words representation of queries, and the output words are the top-k words occuring in the relevant documents to the input query. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import heapq\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import gzip\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Huffman Tree\n",
    "\n",
    "As descirbed in the paper as well, we need to use some efficient approximation of softmax in order to train the maximum likelihood model. I decided to implement the hierarchical softmax based on binary huffman tree. The following two cells are the implementation of Binary Huffman tree. The hierarchical softmax has been implemented based on matrix multiplication. This matrix multiplication based hierarchical softmax provides improvement over traditional hierarchical softmax in which to estimate the probability of word, we follow the path from root node to the leaf node corresponding to that word. Following approach estimates the probabilities for all the output words in one shot. No need to follow the path linearly from the root to leaf node. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, token, freq):\n",
    "        self.token = token\n",
    "        self.freq = freq\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        self.idx = None\n",
    "        \n",
    "    def __lt__(self, other):\n",
    "        return self.freq < other.freq\n",
    "    \n",
    "    def __gt__(self, other):\n",
    "        return self.freq > other.freq\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        if(other == None):\n",
    "            return False\n",
    "        if(not isinstance(other, Node)):\n",
    "            return False\n",
    "        return self.freq == other.freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HuffmanTree:\n",
    "    def __init__(self):\n",
    "        self.heap = []\n",
    "        self.codes = {}\n",
    "        self.reverse_mapping = {}\n",
    "        self.path_tensors = {}\n",
    "        self.selectors = {}\n",
    "        self.token_ids = {}\n",
    "        self.root = None\n",
    "        self.global_path_tensor = None\n",
    "        self.global_selector = None\n",
    "        self.global_splitter = None\n",
    "        self.global_split_map = {}\n",
    "        \n",
    "        \n",
    "    def make_heap(self, frequency):\n",
    "        for key in frequency:\n",
    "            node = Node(key, frequency[key])\n",
    "            heapq.heappush(self.heap, node)\n",
    "            \n",
    "    def merge_nodes(self):\n",
    "        while(len(self.heap)>1):\n",
    "            node1 = heapq.heappop(self.heap)\n",
    "            node2 = heapq.heappop(self.heap)\n",
    "            \n",
    "            merged = Node(None, node1.freq + node2.freq)\n",
    "            merged.left = node1\n",
    "            merged.right = node2\n",
    "            heapq.heappush(self.heap, merged)\n",
    "            \n",
    "    def make_codes_helper(self, root, current_code):\n",
    "        if(root==None):\n",
    "            return\n",
    "        if(root.token != None):\n",
    "            self.codes[root.token] = current_code\n",
    "            self.reverse_mapping[current_code] = root.token\n",
    "            return\n",
    "        \n",
    "        self.make_codes_helper(root.left, current_code + \"0\")\n",
    "        self.make_codes_helper(root.right, current_code + \"1\")\n",
    "        \n",
    "    def make_codes(self):\n",
    "        root = heapq.heappop(self.heap)\n",
    "        self.root = root\n",
    "        current_code = \"\"\n",
    "        self.make_codes_helper(root, current_code)\n",
    "        \n",
    "    def assign_ids(self):\n",
    "        root = self.root\n",
    "        queue = []\n",
    "        queue.append(root)\n",
    "        idx = 0\n",
    "        \n",
    "        while(len(queue)):\n",
    "            root = queue.pop(0)\n",
    "            \n",
    "            if root.left:\n",
    "                queue.append(root.left)\n",
    "            if root.right:\n",
    "                queue.append(root.right)\n",
    "                \n",
    "            if (root.left or root.right):\n",
    "                root.idx = idx \n",
    "                idx +=1\n",
    "                \n",
    "    def assign_path_tensors(self):\n",
    "        for k,v in self.codes.items():\n",
    "            path = []\n",
    "            for c in v:\n",
    "                if(c=='0'):\n",
    "                    path.append(1)\n",
    "                else:\n",
    "                    path.append(-1)\n",
    "            self.path_tensors[k] = torch.FloatTensor(path)\n",
    "            \n",
    "        self.global_path_tensor = torch.FloatTensor(torch.cat(list(self.path_tensors.values()))).cuda()\n",
    "        \n",
    "            \n",
    "    def assign_selectors(self):\n",
    "        for k,v in self.codes.items():\n",
    "            root = self.root\n",
    "            sel = []\n",
    "            sel.append(root.idx)\n",
    "\n",
    "            for c in v[:-1]:\n",
    "                if c=='0':\n",
    "                    root = root.left\n",
    "                else:\n",
    "                    root = root.right\n",
    "                sel.append(root.idx)\n",
    "                \n",
    "            self.selectors[k] = torch.LongTensor(sel)\n",
    "        self.global_selector = torch.LongTensor(torch.cat(list(self.selectors.values()))).cuda()\n",
    "        \n",
    "    def build_global_splitter(self):\n",
    "        splitter = []\n",
    "        for k, v in self.selectors.items():\n",
    "            splitter.append(list(v.shape)[0])\n",
    "        self.global_splitter = splitter\n",
    "        self.global_split_map = dict(zip(list(self.codes.keys()), range(len(self.codes))))\n",
    "        \n",
    "        codes_len_list = [len(code) for code in self.codes.values()]\n",
    "        assert(codes_len_list == self.global_splitter)\n",
    "        \n",
    "    def build(self, vocab):\n",
    "        self.make_heap(vocab)\n",
    "        self.merge_nodes()\n",
    "        self.make_codes()\n",
    "        self.assign_ids()\n",
    "        self.assign_path_tensors()\n",
    "        self.assign_selectors()\n",
    "        self.build_global_splitter()\n",
    "        self.token_ids = dict(zip(range(len(vocab)), list(vocab.keys())))\n",
    "        \n",
    "        for k,v in self.token_ids.items():\n",
    "            assert(k==v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relevance based word embeddings model\n",
    "\n",
    "The code in the following cell is Relevance model which estimates the probabilities of words of being appearing in relevant documents for the given input query. We train this model using Negative Log likelihood and cross entropy loss function, exactly as described in the original work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelevanceModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab):\n",
    "        super(RelevanceModel, self).__init__()\n",
    "        self.vocab_size = len(vocab)\n",
    "        self.embedding_dim = 300\n",
    "        \n",
    "        self.embedding = nn.Linear(self.vocab_size, self.embedding_dim).cuda()\n",
    "        self.hs = nn.Linear(self.vocab_size-1, self.embedding_dim).cuda()\n",
    "        \n",
    "        self.ht = HuffmanTree()\n",
    "        self.ht.build(vocab)\n",
    "        \n",
    "    def forward(self, q, words):\n",
    "        qs = self.embedding(q)\n",
    "        h = torch.mean(qs, axis=0).cuda()\n",
    "        \n",
    "        selector = list(map(self.ht.selectors.__getitem__, words))\n",
    "        splitter = [list(sel.shape)[0] for sel in selector]\n",
    "        \n",
    "        select = torch.cat(selector).cuda()\n",
    "        path = torch.cat(list(map(self.ht.path_tensors.__getitem__, words))).cuda()\n",
    "        \n",
    "        A = self.hs(select)\n",
    "        \n",
    "        dots = torch.matmul(A, h)\n",
    "        apply_paths = dots * path\n",
    "        \n",
    "        apply_sigmoid = torch.sigmoid(apply_paths)\n",
    "        apply_log = -1*torch.log(apply_sigmoid)\n",
    "        \n",
    "        splits = torch.split(apply_log, splitter, dim=0)\n",
    "        \n",
    "        log_probs = torch.stack([torch.sum(split) for split in splits])\n",
    "        \n",
    "        return log_probs\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLoader \n",
    "\n",
    "The dataframe `data_frame.p` is the training data. This data contains for each query, the top 500 words and their probabilities. The probabilities were estimated using the state-of-the-art language modelling approach. There is a seperate notebook for the preparation of the training data from the corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class Rel_dataset(Dataset):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.df = pd.read_pickle('data_frame.p')\n",
    "        self.df.columns = list(range(1000))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        words = self.df.iloc[index][:500].values.astype(np.int64)\n",
    "        probs = self.df.iloc[index][500:].values.astype(np.float32)\n",
    "        name = self.df.iloc[index].name\n",
    "        \n",
    "        return name, words, probs\n",
    "    \n",
    "dataset = Rel_dataset()\n",
    "dataloader = DataLoader(dataset, batch_size = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = pickle.load(open(\"rel_vocab.p\", \"rb\"))\n",
    "vocab = dict(zip(range(len(v)), list(v.values())))\n",
    "\n",
    "model = RelevanceModel(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-1, momentum=0.9)\n",
    "def my_loss(out_probs, probs):\n",
    "    loss = out_probs * probs\n",
    "    tot_loss = torch.sum(loss)\n",
    "    return tot_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "qvecs = torch.load(\"qvecs.p\")\n",
    "num_epochs = 100\n",
    "\n",
    "losses = []\n",
    "PATH = \"model_checkpoints/model\"\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"Epoch : \",epoch)\n",
    "    epoch_loss = 0\n",
    "    for name, words, probs in tqdm(dataloader):    \n",
    "        q = qvecs[name[0]]\n",
    "        out_probs = model(q, words.tolist()[0])\n",
    "        probs = probs.cuda()\n",
    "        loss = my_loss(out_probs, probs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        epoch_loss = epoch_loss + loss.data\n",
    "    \n",
    "        \n",
    "    losses.append(epoch_loss) \n",
    "    print(\"Epoch : \", epoch, \"\\t Loss : \", epoch_loss)\n",
    "    \n",
    "    MODEL_PATH = PATH + str(epoch) + \".pt\"\n",
    "    \n",
    "    torch.save({\n",
    "        'epoch' : epoch,\n",
    "        'model_state_dict' : model.state_dict(),\n",
    "        'optimizer_state_dict' : optimizer.state_dict(),\n",
    "        'loss' : epoch_loss\n",
    "    }, MODEL_PATH)\n",
    "    \n",
    "pickle.dump(losses, open('training_loss.p', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
